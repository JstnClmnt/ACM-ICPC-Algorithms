{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Text-Summarization",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JstnClmnt/ACM-ICPC-Algorithms/blob/master/NLP_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruSXNIBlSo6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import re\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275RJ-74TJUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOsqMIZUAy5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        " \n",
        "def getListOfFiles(dirName):\n",
        "    # create a list of file and sub directories \n",
        "    # names in the given directory \n",
        "    listOfFile = os.listdir(dirName)\n",
        "    allFiles = list()\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "        # Create full path\n",
        "        fullPath = dirName+\"/\"+entry\n",
        "        # If entry is a directory then get the list of files in this directory \n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "                \n",
        "    return allFiles        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4YHEIGIA1wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"/content/drive/Team Drives/AI Lords/NLP-Text-Summarization/dataset.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGafmweQBJXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "dirName = 'data';\n",
        "    \n",
        "# Get the list of all files in directory tree at given path\n",
        "listOfFiles = getListOfFiles(dirName)\n",
        "    \n",
        "\n",
        "df=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"])\n",
        "# Print the files    \n",
        "for elem in listOfFiles:\n",
        "    print(\"Reading: \"+elem)\n",
        "    file1 = open(elem,\"r\",encoding=\"latin-1\") \n",
        "    sampleNews=file1.read().split(\"\\n\")\n",
        "    newsDesc=\"\"\n",
        "    for strline in sampleNews[1:len(sampleNews)]:\n",
        "        newsDesc+=strline\n",
        "    \"\"\"\n",
        "    print(\"Category: \"+elem.split(\"/\")[1])\n",
        "    print(\"Title:\"+sampleNews[0])\n",
        "    print(\"Description:\"+newsDesc[0:50])\n",
        "    \"\"\"\n",
        "    dfsample=pd.DataFrame(columns=[\"Title\",\"Description\",\"Category\"],data=[[sampleNews[0],newsDesc,elem.split(\"/\")[1]]])\n",
        "    df=df.append(dfsample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQmPEP0PBuxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.reset_index()\n",
        "df=df.drop(list(df)[0], axis=1)\n",
        "df[\"Description\"].head()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSpeFwK2BxSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[[\"Title\",\"Description\"]]\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUBzG4lWrCW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Title'] = df['Title'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
        "df['Description'] = df['Description'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
        "df=df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ece8p5BI2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(oov_token=1,num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df[\"Description\"])\n",
        "article_sequences = tokenizer.texts_to_sequences(df[\"Description\"])\n",
        "art_word_index = tokenizer.word_index\n",
        "len(art_word_index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkW7llv3BxJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "art_word_index_1500 = {}\n",
        "counter = 0\n",
        "for word in art_word_index.keys():\n",
        "    if art_word_index[word] == 0:\n",
        "        print(\"found 0!\")\n",
        "        break\n",
        "    if art_word_index[word] > VOCAB_SIZE:\n",
        "        continue\n",
        "    else:\n",
        "        art_word_index_1500[word] = art_word_index[word]\n",
        "        counter += 1\n",
        "print(counter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-Z46WZBCH8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(df[\"Title\"])\n",
        "summary_sequences = tokenizer.texts_to_sequences(df[\"Title\"])\n",
        "sum_word_index = tokenizer.word_index\n",
        "len(sum_word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owWhmVpICXfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_word_index_1500 = {}\n",
        "counter = 0\n",
        "for word in sum_word_index.keys():\n",
        "    if sum_word_index[word] == 0:\n",
        "        print(\"found 0!\")\n",
        "        break\n",
        "    if sum_word_index[word] > VOCAB_SIZE:\n",
        "        continue\n",
        "    else:\n",
        "        sum_word_index_1500[word] = sum_word_index[word]\n",
        "        counter += 1\n",
        "print(counter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdIkHeY5pbOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(article_sequences, summary_sequences, test_size=0.20, random_state=42)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IqWLGE2v3-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E767o2kOv7QC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \"glove.6B.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xl0omApv-UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogkAHIowA3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMB_DIM=300\n",
        "num_words=VOCAB_SIZE+1\n",
        "print(\"Number of Words:\"+str(num_words))\n",
        "\n",
        "#MAX_LENGTH = len(max(X_train, key=len))\n",
        "#print(\"Max Length: \"+str(MAX_LENGTH))  # 271\n",
        "\n",
        "MAX_LENGTH = 500\n",
        "print(\"Max Length for Sentences: \"+str(MAX_LENGTH))\n",
        "MAX_LENGTH_SUM = len(max(y_train, key=len))\n",
        "print(\"Max Length for Summaries: \"+str(MAX_LENGTH_SUM))  # 271\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post',truncating=\"post\")\n",
        "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post',truncating=\"post\")\n",
        "train_sentences_Y= pad_sequences(y_train, maxlen=MAX_LENGTH_SUM, padding='post',truncating=\"post\")\n",
        "test_sentences_Y= pad_sequences(y_test, maxlen=MAX_LENGTH_SUM, padding='post',truncating=\"post\")\n",
        "#y_train=to_categorical(y_train)\n",
        "#y_test=to_categorical(y_test)\n",
        "print(train_sentences_X[0])\n",
        "print(train_sentences_Y[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "difuNX4awHnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_art=np.zeros((num_words,EMB_DIM))\n",
        "numNoEmb=0\n",
        "#print(word2index)\n",
        "for word,i in art_word_index_1500.items():\n",
        "    embedding_vector=embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_art[i]=embedding_vector\n",
        "    else:\n",
        "      numNoEmb+=1\n",
        "print(numNoEmb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxG9s6eFD44S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_sum=np.zeros((num_words,EMB_DIM))\n",
        "#print(word2index)\n",
        "numNoEmb=0\n",
        "for word,i in sum_word_index_1500.items():\n",
        "    if i>num_words:\n",
        "        continue\n",
        "    embedding_vector=embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_sum[i]=embedding_vector\n",
        "    else:\n",
        "      numNoEmb+=1\n",
        "print(numNoEmb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfxnS2FnLFEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import concatenate,Input,Dense, CuDNNLSTM,LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation,Dropout,Add\n",
        "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.initializers import Constant\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "HIDDEN_UNITS=512\n",
        "\n",
        "\"\"\"\n",
        "Chatbot Inspired Encoder-Decoder-seq2seq\n",
        "\"\"\"\n",
        "'''\n",
        "encoder_inputs = Input(shape=(MAX_LENGTH, ))\n",
        "encoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix_art),input_length=MAX_LENGTH,trainable=False,mask_zero=True)(encoder_inputs)\n",
        "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "\n",
        "decoder_inputs = Input(shape=(MAX_LENGTH, ))\n",
        "decoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix_sum),input_length=MAX_LENGTH,trainable=False,mask_zero=True)(decoder_inputs)\n",
        "decoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
        "outputs = TimeDistributed(Dense(num_words, activation='softmax'))(decoder_outputs)\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "'''\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Bidirectional LSTM: Others Inspired Encoder-Decoder-seq2seq\n",
        "\"\"\"\n",
        "encoder_inputs = Input(shape=(MAX_LENGTH,))\n",
        "encoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix_art),input_length=MAX_LENGTH,trainable=False,mask_zero=True)(encoder_inputs)\n",
        "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
        "encoder_LSTM_R = LSTM(HIDDEN_UNITS, return_state=True, go_backwards=True)\n",
        "encoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(encoder_embedding)\n",
        "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "\n",
        "final_h = Add()([state_h, state_h_R])\n",
        "final_c = Add()([state_c, state_c_R])\n",
        "encoder_states = [final_h, final_c]\n",
        "\n",
        "\"\"\"\n",
        "decoder\n",
        "\"\"\"\n",
        "decoder_inputs = Input(shape=(MAX_LENGTH_SUM,))\n",
        "decoder_embedding = Embedding(num_words,EMB_DIM,embeddings_initializer=Constant(embedding_matrix_sum),input_length=MAX_LENGTH_SUM,trainable=False,mask_zero=True)(decoder_inputs)\n",
        "decoder_LSTM = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \n",
        "decoder_dense = TimeDistributed(Dense(num_words, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model= tf.keras.Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model_seq2seq.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmzMab8fTb4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples_train = len(train_sentences_Y)\n",
        "print(num_samples_train)\n",
        "decoder_output_data_train = np.zeros(shape=(num_samples_train, MAX_LENGTH_SUM, num_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UrgAgqZUPg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_samples_test = len(test_sentences_Y)\n",
        "print(num_samples_test)\n",
        "decoder_output_data_test = np.zeros(shape=(num_samples_test, MAX_LENGTH_SUM, num_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvgOtq90XMEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, seqs in enumerate(train_sentences_Y):\n",
        "    for j, seq in enumerate(seqs):\n",
        "        if j > 0:\n",
        "            decoder_output_data_train[i][j][seq] = 1\n",
        "            \n",
        "for i, seqs in enumerate(test_sentences_Y):\n",
        "    for j, seq in enumerate(seqs):\n",
        "        if j > 0:\n",
        "            decoder_output_data_test[i][j][seq] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oGKNZojVHxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "save = ModelCheckpoint('best_model.hdf5', save_best_only=True, monitor='acc', mode='max')\n",
        "es_acc = EarlyStopping(monitor='acc', mode='max',verbose=1,patience=10,min_delta=0.003)\n",
        "es_loss = EarlyStopping(monitor='loss', mode='min',patience=10,verbose=1)\n",
        "history = model.fit([train_sentences_X, train_sentences_Y], \n",
        "                     decoder_output_data_train, \n",
        "                     epochs=150, \n",
        "                     batch_size=32,\n",
        "                     validation_data=([test_sentences_X, test_sentences_Y], decoder_output_data_test),\n",
        "                     callbacks=[save,es_acc,es_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxLCWWiBLObh",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBMwR2U-fIVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure()\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.yticks(np.arange(0,1,step=0.1))\n",
        "plt.savefig(\"acc_lstm.png\")\n",
        "\n",
        "plt.figure( )\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig(\"loss_lstm.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra792qlsSeJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_LSTM(\n",
        "    decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4l1ncfScr94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((MAX_LENGTH_SUM, num_words))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    #target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[ -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > MAX_LENGTH_SUM):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((MAX_LENGTH_SUM, num_words))\n",
        "        target_seq[0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ew3SZ1ctYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "09f530a0-d140-4553-9800-bc62a39a1e8e"
      },
      "source": [
        "print(decode_sequence(train_sentences_X[0:1]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-e8fa03016f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-9bce811a72c2>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         output_tokens, h, c = decoder_model.predict(\n\u001b[0;32m---> 16\u001b[0;31m             [target_seq] + states_value)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_10 to have shape (9,) but got array with shape (20001,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE0svkOsruBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "1690935f-103e-421a-a7a2-06439a7b8caa"
      },
      "source": [
        " it predict the next word again and again.\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "# Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            stop_condition = True\n",
        "# Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "return decoded_sentence"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-ef5d2a36e253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    351\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_9 to have 2 dimensions, but got array with shape ()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7zJ3ncsa-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}